"""
FastAPI Web UI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import Request
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
import json
from typing import List, Optional
import asyncio

from refsys.models import CSLItem
from refsys.ingest import parse_csl_from_dict, parse_csl_from_json_file, deduplicate_items
from refsys.verify import verify_work, Verifier
from refsys.position import PositionAnalyzer, format_position_summary
from refsys.format import ReferenceFormatter, InTextCitation, export_to_bibtex
from refsys.db.dao import WorkDAO, CheckDAO, ClaimCardDAO, ReadEvidenceDAO
from refsys.db.init_db import initialize_database
from refsys.readcheck import ClaimCard, ReadingScorer, ReadingEvidence

app = FastAPI(
    title="RefSys",
    description="æ­£ç¢ºãªå‚è€ƒæ–‡çŒ®ãƒ»å¼•ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬è‡ªå‹•ç”Ÿæˆï¼‹å®Ÿåœ¨æ€§/æ—¢èª­æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ",
    version="0.1.0"
)

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ä¸­...")
    await initialize_database()
    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†!")

# CORSè¨­å®šï¼ˆNext.jsãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # Next.jsé–‹ç™ºã‚µãƒ¼ãƒãƒ¼
        "http://127.0.0.1:3000",
        "http://localhost:8000",  # FastAPIè‡ªèº«
        "https://seisei-a-ichaker.vercel.app",  # Vercelæœ¬ç•ªç’°å¢ƒ
    ],
    allow_credentials=True,
    allow_methods=["*"],  # GET, POST, PUT, DELETEç­‰ã™ã¹ã¦è¨±å¯
    allow_headers=["*"],  # ã™ã¹ã¦ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨±å¯
)

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã‚¹ã‚¿ãƒ†ã‚£ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«
BASE_DIR = Path(__file__).resolve().parent
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))

# ã‚¹ã‚¿ãƒ†ã‚£ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
static_dir = BASE_DIR / "static"
static_dir.mkdir(exist_ok=True)


@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸"""
    works = await WorkDAO.list_all(limit=50)
    return templates.TemplateResponse(
        "index.html",
        {"request": request, "works": works}
    )


@app.get("/works", response_class=HTMLResponse)
async def list_works(request: Request, limit: int = 50, offset: int = 0):
    """æ–‡çŒ®ãƒªã‚¹ãƒˆ"""
    works = await WorkDAO.list_all(limit=limit, offset=offset)
    return templates.TemplateResponse(
        "works_list.html",
        {"request": request, "works": works}
    )


@app.get("/works/{work_id}", response_class=HTMLResponse)
async def work_detail(request: Request, work_id: str):
    """æ–‡çŒ®è©³ç´°"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    # æ¤œè¨¼çµæœå–å¾—
    checks = await CheckDAO.get_by_work(work_id)
    
    # æ—¢èª­è¨¼è·¡å–å¾—
    evidences = await ReadEvidenceDAO.get_by_work(work_id)
    
    # ã‚«ãƒ¼ãƒ‰å–å¾—
    cards = await ClaimCardDAO.get_by_work(work_id)
    
    # æ—¢èª­ã‚¹ã‚³ã‚¢è¨ˆç®—
    scorer = ReadingScorer()
    read_evidences = [
        ReadingEvidence(
            work_id=e['work_id'],
            pdf_path=e['pdf_path'],
            page=e['page'],
            dwell_secs=e['dwell_secs'],
            coverage=e['coverage']
        )
        for e in evidences
    ]
    
    claim_cards = [
        ClaimCard(
            work_id=c['work_id'],
            claim=c['claim'],
            evidence_snippet=c['evidence_snippet'],
            page_from=c['page_from'],
            page_to=c['page_to'],
            limitations=c['limitations'],
            verified=c['verified'],
            card_id=c['id']
        )
        for c in cards
    ]
    
    # ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®šï¼ˆä»®ï¼‰
    total_pages = max([e['page'] for e in evidences], default=0) + 1
    
    reading_score = scorer.calculate_score(
        evidences=read_evidences,
        total_pages=total_pages,
        cards=claim_cards
    ) if evidences else None
    
    return templates.TemplateResponse(
        "work_detail.html",
        {
            "request": request,
            "work": work,
            "checks": checks,
            "evidences": evidences,
            "cards": cards,
            "reading_score": reading_score
        }
    )


# ============================================
# JSON API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆNext.jsãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ï¼‰
# ============================================

@app.get("/api/works")
async def api_list_works(limit: Optional[int] = 100):
    """æ–‡çŒ®ãƒªã‚¹ãƒˆï¼ˆJSONï¼‰"""
    if limit is None:
        limit = 100
    works = await WorkDAO.list_all(limit=limit)
    return works


@app.get("/api/works/{work_id}")
async def api_get_work(work_id: str):
    """æ–‡çŒ®è©³ç´°ï¼ˆJSONï¼‰"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    return work


@app.get("/api/works/{work_id}/checks")
async def api_get_checks(work_id: str):
    """æ¤œè¨¼çµæœå–å¾—ï¼ˆJSONï¼‰"""
    checks = await CheckDAO.get_by_work(work_id)
    return checks


@app.get("/api/works/{work_id}/cards")
async def api_get_cards(work_id: str):
    """å¼•ç”¨ã‚«ãƒ¼ãƒ‰å–å¾—ï¼ˆJSONï¼‰"""
    cards = await ClaimCardDAO.get_by_work(work_id)
    return cards


@app.get("/api/works/{work_id}/reading-score")
async def api_get_reading_score(work_id: str):
    """æ—¢èª­ã‚¹ã‚³ã‚¢å–å¾—ï¼ˆJSONï¼‰"""
    evidences = await ReadEvidenceDAO.get_by_work(work_id)
    cards = await ClaimCardDAO.get_by_work(work_id)
    
    scorer = ReadingScorer()
    read_evidences = [
        ReadingEvidence(
            work_id=e['work_id'],
            pdf_path=e.get('pdf_path', ''),
            page=e.get('page', 0),
            dwell_secs=e.get('dwell_secs', 0),
            coverage=e.get('coverage', 0.0)
        )
        for e in evidences
    ]
    
    claim_cards = [
        ClaimCard(
            work_id=c['work_id'],
            claim_text=c['claim_text'],
            context=c.get('context', ''),
            page_numbers=c.get('page_numbers', '')
        )
        for c in cards
    ]
    
    total_pages = max([e.get('page', 0) for e in evidences], default=0) + 1
    
    score_data = scorer.calculate_score(
        evidences=read_evidences,
        total_pages=total_pages,
        cards=claim_cards
    ) if evidences or cards else {
        'score': 0,
        'card_count': len(cards),
        'evidence_count': len(evidences),
        'passed': False
    }
    
    return score_data


@app.post("/api/works/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡çŒ®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    import fitz  # PyMuPDF
    from datetime import datetime
    import tempfile
    import os
    import traceback
    
    try:
        if not file.filename.endswith('.pdf'):
            raise HTTPException(status_code=400, detail="PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        try:
            # PDFã‚’é–‹ã
            doc = fitz.open(tmp_file_path)
            metadata = doc.metadata
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            title = metadata.get('title', '') or file.filename.replace('.pdf', '')
            author_str = metadata.get('author', '')
            
            # è‘—è€…æƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹
            authors = []
            if author_str:
                # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã¾ãŸã¯ã‚»ãƒŸã‚³ãƒ­ãƒ³åŒºåˆ‡ã‚Šã‚’æƒ³å®š
                author_parts = author_str.replace(';', ',').split(',')
                for part in author_parts:
                    part = part.strip()
                    if part:
                        # "Family, Given" or "Given Family" å½¢å¼ã«å¯¾å¿œ
                        if ' ' in part:
                            names = part.split()
                            authors.append({
                                'family': names[-1],
                                'given': ' '.join(names[:-1])
                            })
                        else:
                            authors.append({'family': part, 'given': ''})
            
            if not authors:
                authors = [{'family': 'Unknown', 'given': ''}]
            
            # ç™ºè¡Œå¹´ã‚’æŠ½å‡ºï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ä½œæˆæ—¥ã‹ã‚‰ï¼‰
            issued_year = None
            if metadata.get('creationDate'):
                try:
                    # "D:20231201..." å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
                    date_str = metadata['creationDate']
                    if date_str.startswith('D:'):
                        year_str = date_str[2:6]
                        issued_year = int(year_str)
                except:
                    pass
            
            if not issued_year:
                issued_year = datetime.now().year
            
            # ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—ï¼ˆdocã‚’é–‰ã˜ã‚‹å‰ã«ï¼‰
            page_count = len(doc)
            
            doc.close()
            
            # ä½ç½®ã¥ã‘åˆ†æï¼ˆCSLItemã‚’dictå½¢å¼ã«å¤‰æ›ï¼‰
            analyzer = PositionAnalyzer()
            work_dict = {
                'id': f"pdf_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                'type': 'article',
                'title': title,
                'author': authors,
                'issued': {'date-parts': [[issued_year]]},
                'abstract': metadata.get('subject', '')
            }
            position = await analyzer.analyze_work(work_dict)
            
            # CSL-JSONå½¢å¼ã§ä½œæˆï¼ˆä½ç½®ã¥ã‘æƒ…å ±ã‚’å«ã‚€ï¼‰
            csl_item = CSLItem(
                id=work_dict['id'],
                type='article',
                title=title,
                author=authors,
                issued={'date-parts': [[issued_year]]},
                abstract=metadata.get('subject', ''),
                peer_reviewed=position.peer_reviewed,
                consensus_score=position.consensus_score
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            work_id = await WorkDAO.create(csl_item)
            
            return {
                'work_id': work_id,
                'title': title,
                'authors': authors,
                'year': issued_year,
                'pages': page_count,
                'message': 'PDFã‹ã‚‰æ–‡çŒ®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ'
            }
            
        except Exception as e:
            error_detail = f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}\n{traceback.format_exc()}"
            print(error_detail)  # ãƒ­ã‚°ã«å‡ºåŠ›
            raise HTTPException(status_code=500, detail=error_detail)
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(tmp_file_path):
                os.remove(tmp_file_path)
    
    except HTTPException:
        raise
    except Exception as e:
        error_detail = f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}\n{traceback.format_exc()}"
        print(error_detail)  # ãƒ­ã‚°ã«å‡ºåŠ›
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/api/works/import")
async def import_works(
    file: Optional[UploadFile] = File(None),
    json_data: Optional[str] = Form(None)
):
    """æ–‡çŒ®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆCSL-JSONï¼‰"""
    items = []
    
    if file:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        content = await file.read()
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯åˆ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æ¡ˆå†…
        if file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400, 
                detail="PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ /api/works/upload-pdf ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„"
            )
        
        data = json.loads(content.decode('utf-8'))
        
        if isinstance(data, list):
            for item_data in data:
                items.append(parse_csl_from_dict(item_data))
        elif isinstance(data, dict):
            items.append(parse_csl_from_dict(data))
    
    elif json_data:
        # ãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰ã®JSON
        data = json.loads(json_data)
        items.append(parse_csl_from_dict(data))
    
    if not items:
        raise HTTPException(status_code=400, detail="No data provided")
    
    # é‡è¤‡æ’é™¤
    unique_items, duplicates = deduplicate_items(items)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    created_ids = []
    for item in unique_items:
        try:
            # ä½ç½®ã¥ã‘åˆ†æ
            analyzer = PositionAnalyzer()
            position = await analyzer.analyze_work(item.to_dict())
            
            item.peer_reviewed = position.peer_reviewed
            item.consensus_score = position.consensus_score
            
            # ä¿å­˜
            work_id = await WorkDAO.create(item)
            created_ids.append(work_id)
            
            # å®Ÿåœ¨æ€§æ¤œè¨¼ã‚’éåŒæœŸã§å®Ÿè¡Œ
            asyncio.create_task(verify_and_save(work_id, item.to_dict()))
        
        except Exception as e:
            print(f"Error importing work: {e}")
    
    return {
        "imported": len(created_ids),
        "duplicates": len(duplicates),
        "work_ids": created_ids
    }


async def verify_and_save(work_id: str, work_data: dict):
    """æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¦ä¿å­˜"""
    try:
        results = await verify_work(work_data)
        
        for kind, result in results.items():
            await CheckDAO.create(
                work_id=work_id,
                kind=result.kind,
                status=result.status,
                detail=result.detail,
                http_code=result.http_code
            )
    except Exception as e:
        print(f"Verification error for {work_id}: {e}")


@app.post("/api/works/{work_id}/verify")
async def api_verify_work(work_id: str):
    """æ–‡çŒ®ã®å®Ÿåœ¨æ€§æ¤œè¨¼ï¼ˆJSONï¼‰"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    # CSL-JSONã‚’å¾©å…ƒ
    csl_data = json.loads(work['raw_csl_json'])
    
    # æ¤œè¨¼å®Ÿè¡Œ
    results = await verify_work(csl_data)
    
    # ä¿å­˜
    for kind, result in results.items():
        await CheckDAO.create(
            work_id=work_id,
            kind=result.kind,
            status=result.status,
            detail=result.detail,
            http_code=result.http_code
        )
    
    return {
        "work_id": work_id,
        "results": {k: v.to_dict() for k, v in results.items()}
    }


@app.post("/api/works/{work_id}/cards")
async def api_create_claim_card(
    work_id: str,
    claim_text: str = Form(...),
    context: str = Form(...),
    page_numbers: Optional[str] = Form(None)
):
    """å¼•ç”¨ã‚«ãƒ¼ãƒ‰ä½œæˆï¼ˆJSONï¼‰"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    card_id = await ClaimCardDAO.create(
        work_id=work_id,
        claim_text=claim_text,
        context=context,
        page_numbers=page_numbers
    )
    
    card = await ClaimCardDAO.get(card_id)
    return card


@app.post("/api/works/{work_id}/reading-evidence")
async def api_submit_reading_evidence(
    work_id: str,
    page_numbers: str = Form(...),
    notes: Optional[str] = Form(None)
):
    """æ—¢èª­è¨¼è·¡æå‡ºï¼ˆJSONï¼‰"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ãƒ‘ãƒ¼ã‚¹
    pages = []
    for part in page_numbers.split(','):
        part = part.strip()
        if '-' in part:
            start, end = map(int, part.split('-'))
            pages.extend(range(start, end + 1))
        else:
            pages.append(int(part))
    
    # å„ãƒšãƒ¼ã‚¸ã®è¨¼è·¡ã‚’è¨˜éŒ²
    evidence_ids = []
    for page in pages:
        evidence_id = await ReadEvidenceDAO.create(
            work_id=work_id,
            pdf_path='',  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            page=page,
            dwell_secs=60,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            coverage=0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        )
        evidence_ids.append(evidence_id)
    
    return {
        "work_id": work_id,
        "pages": pages,
        "evidence_count": len(evidence_ids)
    }


@app.get("/api/works/{work_id}/cite")
async def api_get_citation(work_id: str, format: str = 'apa'):
    """å¼•ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆJSONï¼‰"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    csl_json = json.loads(work['raw_csl_json'])
    
    formatter = ReferenceFormatter()
    
    if format == 'bibtex':
        citation = export_to_bibtex([csl_json])
    else:
        # CSLItemã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from refsys.ingest import parse_csl_from_dict
        csl_item = parse_csl_from_dict(csl_json)
        # format_referenceãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼ˆAPAå½¢å¼ï¼‰
        citation = formatter.format_reference(csl_item)
    
    return {"citation": citation}


@app.post("/api/export/bibliography")
async def api_export_bibliography(
    work_ids: List[str],
    format: str = 'apa'
):
    """å‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰"""
    from refsys.ingest import parse_csl_from_dict
    
    csl_items = []
    for work_id in work_ids:
        work = await WorkDAO.get(work_id)
        if work:
            csl_json = json.loads(work['raw_csl_json'])
            csl_item = parse_csl_from_dict(csl_json)
            csl_items.append(csl_item)
    
    formatter = ReferenceFormatter()
    
    if format == 'bibtex':
        bibliography = export_to_bibtex([item.to_dict() for item in csl_items])
    else:
        # format_bibliographyãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼ˆAPAå½¢å¼ï¼‰
        bibliography = formatter.format_bibliography(csl_items)
    
    return {"bibliography": bibliography}


# ============================================
# ä»¥ä¸‹ã€æ—¢å­˜ã®HTMLã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================

@app.post("/works/{work_id}/verify")
async def verify_work_endpoint(work_id: str):
    """æ–‡çŒ®ã®å®Ÿåœ¨æ€§æ¤œè¨¼"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    # CSL-JSONã‚’å¾©å…ƒ
    csl_data = json.loads(work['raw_csl_json'])
    
    # æ¤œè¨¼å®Ÿè¡Œ
    results = await verify_work(csl_data)
    
    # ä¿å­˜
    for kind, result in results.items():
        await CheckDAO.create(
            work_id=work_id,
            kind=result.kind,
            status=result.status,
            detail=result.detail,
            http_code=result.http_code
        )
    
    return {
        "work_id": work_id,
        "results": {k: v.to_dict() for k, v in results.items()}
    }


@app.post("/works/{work_id}/cards")
async def create_claim_card(
    work_id: str,
    claim: str = Form(...),
    evidence_snippet: str = Form(...),
    page_from: int = Form(...),
    page_to: Optional[int] = Form(None),
    limitations: Optional[str] = Form(None)
):
    """å¼•ç”¨æ–‡è„ˆã‚«ãƒ¼ãƒ‰ä½œæˆ"""
    card = ClaimCard(
        work_id=work_id,
        claim=claim,
        evidence_snippet=evidence_snippet,
        page_from=page_from,
        page_to=page_to,
        limitations=limitations
    )
    
    card_id = await ClaimCardDAO.create(
        card_id=card.id,
        work_id=card.work_id,
        claim=card.claim,
        evidence_snippet=card.evidence_snippet,
        page_from=card.page_from,
        page_to=card.page_to,
        limitations=card.limitations,
        verified=card.verified
    )
    
    return {"card_id": card_id, "is_complete": card.is_complete()}


@app.get("/export/bibliography")
async def export_bibliography(
    style: str = "apa",
    format: str = "text",
    limit: int = 100
):
    """å‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    works = await WorkDAO.list_all(limit=limit)
    
    # CSL-JSONã«å¤‰æ›
    items = []
    for work in works:
        csl_data = json.loads(work['raw_csl_json'])
        items.append(CSLItem(**csl_data))
    
    if format == "bibtex":
        output = export_to_bibtex(items)
        return {"format": "bibtex", "content": output}
    
    else:
        # text (APA or IEEE)
        formatter = ReferenceFormatter(style)
        output = formatter.format_bibliography(items)
        return {"format": style, "content": output}


@app.get("/api/works/{work_id}/cite")
async def get_citation(work_id: str, style: str = "apa", page: Optional[str] = None):
    """æœ¬æ–‡ä¸­å¼•ç”¨ã®å–å¾—"""
    work = await WorkDAO.get(work_id)
    if not work:
        raise HTTPException(status_code=404, detail="Work not found")
    
    csl_data = json.loads(work['raw_csl_json'])
    csl = CSLItem(**csl_data)
    
    citation = InTextCitation(style)
    cite_text = citation.cite(csl, page)
    
    return {"citation": cite_text, "style": style}


@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "ok", "version": "0.1.0"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
